# ğŸ“… DAY 3 (11/01/26) â€“ PySpark Transformations Deep Dive

## ğŸ“Œ Introduction
Day 3 focused on mastering **PySpark transformations** used in real-world data engineering workflows. The emphasis was on understanding how PySpark differs from Pandas, performing complex joins, applying window functions for analytical use cases, and creating derived features on large-scale e-commerce data.

---

## ğŸ“š What I Learned
- Differences between **PySpark and Pandas** in terms of scalability and performance
- How to use **inner, left, right, and outer joins** in PySpark
- Applying **window functions** for running totals and ranking
- Creating **derived features** using built-in Spark functions
- Why avoiding unnecessary UDFs improves performance

---

## ğŸ› ï¸ Tasks Performed
1. Loaded the full e-commerce dataset into PySpark  
2. Performed complex joins across multiple datasets  
3. Calculated running totals and rankings using window functions  
4. Created derived features for downstream analytics  

## ğŸ§  Key Takeaways
- PySpark is designed for distributed processing and large datasets  
- Window functions simplify advanced analytical transformations  
- Efficient joins are critical for scalable data pipelines  
- Built-in Spark functions are more optimized than UDFs  


## ğŸ“– Learning Resources
- PySpark Window Functions  
  https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/window.html  
- PySpark SQL Functions  
  https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html  

## ğŸ™ Acknowledgement
Shoutout to @Databricks @Codebasics @IndianDataClub for this challenge.

## ğŸ·ï¸ Tags
- DatabricksWithIDC
- Databricks
- Codebasics
- IndianDataClub
