# ğŸ“… Day 12 â€“ MLflow Basics

## ğŸ“Œ Overview
Day 12 focused on understanding MLflow and its role in managing the machine learning lifecycle. The objective was to learn how to track experiments, log models, and compare results in a reproducible and structured way.

---

## ğŸ“˜ What I Learned

- **MLflow Components**  
  Learned about MLflow Tracking for experiments, MLflow Models for packaging models, and the Model Registry for versioning and lifecycle management.

- **Experiment Tracking**  
  Understood how logging parameters and metrics enables systematic comparison of multiple model runs.

- **Model Logging**  
  Learned how trained models can be stored along with metadata to support reuse and deployment.

- **MLflow UI**  
  Explored the MLflow UI to visualize experiment history, compare runs, and analyze performance metrics.

---

## ğŸ› ï¸ Tasks Performed

- Trained a simple regression model
- Logged model parameters and evaluation metrics
- Logged the trained model using MLflow
- Compared multiple experiment runs using the MLflow UI

---

## ğŸ”‘ Key Takeaways

- Experiment tracking is essential for reproducible ML workflows
- MLflow simplifies comparison between model runs
- Model logging enables better collaboration and governance
- MLflow acts as a foundation for scalable MLOps practices

---

## ğŸš€ Outcome
Built a clear understanding of MLflow fundamentals and how it supports experiment tracking and model management within Databricks.

---

## ğŸ™Œ Acknowledgements

- @Indian Data Club
- @Databricks
- @Codebasics
- #DatabricksWithIDC
